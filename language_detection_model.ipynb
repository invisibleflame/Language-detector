{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language detection model",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD1vBtRJrIQR"
      },
      "source": [
        "!pip install wikipedia unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxjQ3koGToP3"
      },
      "source": [
        "#CONFIG\n",
        "max_letters = 12\n",
        "language_tags = {\n",
        "\n",
        "                'en':['actor', 'alcohol', 'cheque', 'cancer', 'chocolate', 'debate', 'hobby', 'melon', 'propaganda',\n",
        "                      'religion', 'violin', 'england', 'MediaWiki','football','Napoleon','Money','Tiger','eagle','cricket','Finance','military','government','Emotion'],\n",
        "\n",
        "                'cs': ['praha', 'evropa', 'pyreneje', 'voda', 'housle', 'Náboženství', 'Příroda', 'Ekosystém',\n",
        "                    'vzdělání', 'Dům', 'Zpěvák', 'Zeus', 'Mykény', 'Starověké_Řecko', 'Renesance',\n",
        "                    'Andrej_Babiš', 'Správa_železniční_dopravní_cesty', 'Kraje_v_Česku', 'Česko', 'Slezsko',\n",
        "                    'Latina', 'Spojené_království', 'Římský_senát'],\n",
        "\n",
        "                'de': ['Deutsche_Sprache', 'Deutschland', 'Kommunistische_Partei_der_Sowjetunion', 'Wasser',\n",
        "                    'Festkörper', 'Seele', 'Geist', 'Dreifaltigkeit', 'Große', 'Christentum', 'Gott'],\n",
        "\n",
        "                'sv': ['Svenska', 'Sverige', 'Danmark', 'Europeiska_unionen', 'Medeltiden', 'Feodalism', 'Kung',\n",
        "                    'Kejsare', 'Monarki', 'Valmonarki', 'Choklad', 'Mjölk', 'Prolaktin', 'Kvinna', 'Eldvapen',\n",
        "                    'Kina', 'Götar', 'Romantiken', 'Ideologi', 'Tänkande', 'Pedagogik', 'Sekund', 'Solen', 'Väder',\n",
        "                    'Mellanöstern', 'Väte', 'Anatomi', 'Hjärta', 'Puls', 'Grekiska', 'Cypern'],\n",
        "\n",
        "                'fr': ['Français', 'Langues_romanes', 'Charlemagne', 'Traité_de_Verdun', 'Louis_le_Pieux',\n",
        "                    'Son_(physique)', 'Zoologie', 'Intelligence_animale', 'Intelligence', 'Tautologie',\n",
        "                    'Pléonasme', 'Figure_de_style']\n",
        "\n",
        "                 }"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM8U0VDskLcq"
      },
      "source": [
        "import re\n",
        "import wikipedia as wiki\n",
        "from unidecode import unidecode\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def process(page_content, max_word_length):\n",
        "    words = re.sub(r'[^a-zA-Z ]', '', page_content)\n",
        "    lower = words.lower()\n",
        "    word_list = lower.split()\n",
        "    short_words = []\n",
        "    for word in word_list:\n",
        "        if len(word) <= max_word_length:\n",
        "            short_words.append(word)\n",
        "    return short_words\n",
        "\n",
        "def generate_dictionary(tag, max_word_length):\n",
        "\n",
        "    wiki.set_lang(tag)\n",
        "    for topic in language_tags[tag]:\n",
        "        page = wiki.WikipediaPage(topic)\n",
        "        content = page.content\n",
        "        content = unidecode(content)\n",
        "        lst = process(content, max_word_length)\n",
        "\n",
        "    return lst\n",
        "\n",
        "def convert_dic_to_vector(dic, max_word_length):\n",
        "    new_list = []\n",
        "    for word in dic:\n",
        "        vec = ''\n",
        "        n = len(word)\n",
        "        for i in range(n):\n",
        "            current_letter = word[i]\n",
        "            ind = ord(current_letter)-97\n",
        "            placeholder = (str(0)*ind) + str(1) + str(0)*(25-ind)\n",
        "            vec = vec + placeholder\n",
        "        if n < max_word_length:\n",
        "            excess = max_word_length-n\n",
        "            vec = vec +str(0)*26*excess\n",
        "        new_list.append(vec)\n",
        "    print(len(new_list))\n",
        "    return new_list\n",
        "\n",
        "def create_output_vector(tag_index, number_of_languages):\n",
        "    out = str(0)*tag_index + str(1) + str(0)*(number_of_languages-1-tag_index)\n",
        "    return out\n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amC_bo3bkmW4"
      },
      "source": [
        "word_data = []\n",
        "language_data = []\n",
        "master_dic = []\n",
        "\n",
        "count = 0\n",
        "\n",
        "for tag in language_tags.keys():\n",
        "    print('generating dictionary for ' + tag)\n",
        "    dic = generate_dictionary(tag, max_letters)\n",
        "    for word in dic:\n",
        "        master_dic.append(word)\n",
        "    vct = convert_dic_to_vector(dic, max_letters)\n",
        "    for vector in vct:\n",
        "        word_data.append(vector)\n",
        "    output_vct = create_output_vector(count, len(language_tags))\n",
        "    for i in range(len(vct)):\n",
        "        language_data.append(output_vct)\n",
        "    count += 1\n",
        "arr = []\n",
        "for i in range(len(word_data)):\n",
        "    entry = []\n",
        "    entry.append(master_dic[i])\n",
        "    for digit in language_data[i]:\n",
        "        entry.append(float(digit))\n",
        "    for digit in word_data[i]:\n",
        "        entry.append(float(digit))\n",
        "    arr.append(entry)\n",
        "\n",
        "\n",
        "arr = np.array(arr)\n",
        "np.save('arr.npy', arr)\n",
        "df=pd.DataFrame(arr)\n",
        "df.to_csv('data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZalzECnpu5g",
        "outputId": "22aa23f1-578c-4df7-90da-bf009f4e4e96"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "data = arr\n",
        "\n",
        "inputs = data[:, 1+len(language_tags):]\n",
        "labels = data[:, 1:1+len(language_tags)]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.15)\n",
        "\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5681, 312)\n",
            "(5681, 5)\n",
            "(32190, 312)\n",
            "(32190, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_q4jgN3u3xH"
      },
      "source": [
        "import tensorflow as tf\n",
        "x_train=tf.strings.to_number(x_train, out_type=tf.dtypes.float32, name=None)\n",
        "y_test=tf.strings.to_number(y_test, out_type=tf.dtypes.float32, name=None)\n",
        "x_test=tf.strings.to_number(x_test, out_type=tf.dtypes.float32, name=None)\n",
        "y_train=tf.strings.to_number(y_train, out_type=tf.dtypes.float32, name=None)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvu4mfovqDX6"
      },
      "source": [
        "network = Sequential()\n",
        "network.add(Dense(200, input_dim=26*max_letters, activation='tanh'))\n",
        "network.add(Dense(150, activation='tanh'))\n",
        "network.add(Dense(100, activation='tanh'))\n",
        "network.add(Dense(100, activation='tanh'))\n",
        "network.add(Dense(len(language_tags), activation='softmax'))\n",
        "\n",
        "network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mET8AyuD1AzD"
      },
      "source": [
        "!rm -rf ./logs/"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynbgWfbRsZr4"
      },
      "source": [
        "filepath = \"weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "tboard = TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
        "callbacks_list = [checkpoint, tboard]\n",
        "\n",
        "network.fit(x_train, y_train, epochs=100, batch_size=1000, validation_data=(x_test, y_test), callbacks=callbacks_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH6GTXD8vnTe"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqezRDeDzJ5U"
      },
      "source": [
        "network.load_weights('/content/weights.hdf5')\n",
        "while True:\n",
        "    dic = []\n",
        "    valid = False\n",
        "    while not valid:\n",
        "        word = input('Enter word to predict:\\n')\n",
        "        if len(word) <= max_letters:\n",
        "            word = word.lower()\n",
        "            valid = True\n",
        "        else:\n",
        "            print('Word must be less than ' + str(max_letters + 1) + ' letters long')\n",
        "    dic.append(word)\n",
        "    vct_str = convert_dic_to_vector(dic, max_letters)\n",
        "    vct = np.zeros((1, 26 * max_letters))\n",
        "    count = 0\n",
        "    for digit in vct_str[0]:\n",
        "        vct[0,count] = int(digit)\n",
        "        count += 1\n",
        "    prediction_vct = network.predict(vct)\n",
        "\n",
        "    langs = list(language_tags.keys())\n",
        "    for i in range(len(language_tags)):\n",
        "        lang = langs[i]\n",
        "        score = prediction_vct[0][i]\n",
        "        print(lang + ': ' + str(round(100*score, 2)) + '%')\n",
        "    print('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO3sbJsUAVYT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}